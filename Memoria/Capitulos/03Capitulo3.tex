%---------------------------------------------------------------------
%
%                          Capítulo 3
%
%---------------------------------------------------------------------

\chapter{Herramientas}

\begin{resumen}
En este capítulo se profundizará en las herramientas que utilizaremos a lo largo del trabajo. En la sección 3.1 se presenta el diccionario que vamos a utilizar para el marcado emocional. En la sección 3.2 se introduce el framework que vamos a utilizar para el desarrollo de los servicios web, Django. En la sección 3.3 se explica cómo vamos a utilizar Trello para seguir la metodología Scrum. En la sección 3.4 se expone la forma de realizar las pruebas utilizando Jenkins y Doctest. En la sección 3.5 se presentan SpaCy y PyStemmer, las herramientas que se utilizarán para poder procesar las palabras que forman una frase para poder realizar el análisis emocional sobre ella.
\end{resumen}


%-------------------------------------------------------------------
\section{Diccionario}
%-------------------------------------------------------------------
\label{cap3:sec:diccionario}

Este diccionario, basado en el diccionario ANEW traducido al castellano, es una adaptación en el que se han elegido las 5 emociones básicas (tristeza, miedo, alegría, enfado y sorpresa) y la neutralidad para no expresar ninguna emoción. Para crearlo participaron 10 personas; cada una de ellas marcó cada una de las 1034 palabras con una emoción básica o la neutralidad.
Cada palabra puede representar varias emociones por lo que se ha creado un número de confianza \textit{c}, entre e 0 y el 1, que determina con qué certeza la palabra corresponde a esa emoción; es decir indica el tanto por uno de evaluadores que asignaron dicha emoción a la palabra. La suma de estos números debe sumar 1.

	\figura{Bitmap/Capitulo3/fragmento}{width=.9\textwidth}{fig:diccionario}{Fragmento de la adaptación del diccionario ANEW traducido.}
	
En la Tabla \ref{fig:diccionario}, podemos ver los valores obtenidos para las palabras "`abandonado"', "`aborto"' y "`abeja"'. En este ejemplo se puede ver que tristeza fue la emoción asignada a la palabra "`abandonado"' por todos los evaluadores, mientras que con "`abejas"' y "`aborto"' no hubo acuerdo. En el caso de "`aborto"', un 67\% de los anotadores le asignaron tristeza, mientras que un 17\% consideró que la palabra no tenía emoción asociada y un 17\% le asignaron miedo. 

%-------------------------------------------------------------------
\section{Django}
%-------------------------------------------------------------------
\label{cap3:sec:django}

Toda la implementación del trabajo se hará utilizando Django, un framework para aplicaciones web gratuito y open source escrito en Python. 

El framework de Django nos proporciona un servidor web, en el que se almacena la base de datos.
Esta base de datos, contiene las palabras con sus respectivas probabilidades para cada emoción y la neutralidad modeladas mediante su lexema y los grados de certeza para cada emoción. La base de datos permitirá hacer las consultas necesarias. 
Para realizar las diferentes consultas sobre las palabras disponibles existen una serie de clases que implementan los diferentes métodos de un servicio web REST típico: \textbf{GET, POST, DELETE}. Principalmente \textbf{GET}, ya que es este método el que devuelve información sobre la palabra o alguno de sus atributos. 
Cada una de las diferentes clases nos aportarán una manera diferente de acceder a la información, como pueden ser: acceso a todo el diccionario de palabras, a una palabra concreta o a un campo de una palabra concreta.
Los resultados serán devueltos en formato JSON.

%-------------------------------------------------------------------
\section{Trello}
%-------------------------------------------------------------------
\label{cap3:sec:trello}

Trello es una aplicación web que permite organizar proyectos y actividades. Para representar las tareas  y las historias de usuario se usan tarjetas virtuales. En la Figura \ref{fig:sprint} podemos ver el estado inicial del proyecto. Se observa el \textbf{Product Backlog}, del que product owner saca la cantidad de las historias de usuario que quiere que se realicen durante el sprint y estas pasan a la lista \textbf{To Do}. En la Figura \ref{fig:sprint2} se puede ver un ejemplo más avanzado en el que se puede ver como las historias de usuario han sido divididas en tareas para formar el \textbf{Sprint Backlog}, del que van saliendo en orden para estar \textbf{En Progreso} y, una vez acabadas, \textbf{Done}.

	\figura{Bitmap/Capitulo3/Sprint1Plan}{width=.9\textwidth}{fig:sprint}{Sprint inicial que ilustra la estructura.}
	
	\figura{Bitmap/Capitulo3/Sprint1Fin}{width=.9\textwidth}{fig:sprint2}{Final del sprint inicial.}

%-------------------------------------------------------------------
\section{Doctest y Jenkins}
%-------------------------------------------------------------------
\label{cap3:sec:pruebas}

Como ya se comentó en el capítulo 2.4 utilizaremos Jenkins como parte de la integración continua del proyecto. Esto nos permitirá asegurarnos de que la unificación es correcta y realizar las pruebas automáticas. Esto último se llevará a cabo mediante una orden shell que Jenkins ejecutará cada vez que se detecte un cambio en el repositorio. La orden únicamente se encarga de ejecutar el script de pruebas que contendrá las llamadas a los diferentes programas de pruebas que se desarrollen.

Los programas de pruebas utilizarán Doctest para hacer las pruebas. Doctest es un módulo incluido en la librería estandar de Python. Su funcionamiento se basa en definir la función que se quiera probar y, dentro de un comentario al inicio de esta, poner una serie de llamadas y el resultado que se espera obtener de ellas. Tiene una función "`testmod"' que realiza las pruebas y devuelve el número de fallos y el resultado de todas las pruebas. Si el número de fallos es mayor que cero provocamos una excepción que Jenkins detectará para notificar a todo el equipo que hay algún fallo. Los resultados de las pruebas se muestran por consola al acabar y Jenkins los guardará para ayudar a encontrar el problema.

%-------------------------------------------------------------------
\section{SpaCy y PyStemmer}
%-------------------------------------------------------------------
\label{cap3:sec:lematizacion}

El objetivo final es llegar a interpretar textos enteros, no sólo palabras. Para ello se necesita una herramienta que nos facilite trabajar con frases, etiquetando cada una de las palabras que las forman. \textbf{SpaCy} es una librería open source escrita en Python y dedicada al procesamiento de lenguajes naturales. Soporta el español y nos permite etiquetar las palabras para poder buscar sólo aquellas que puedan tener carga emocional.
SpaCy recibirá el texto plano, en este caso una serie de frases, y devolverá un objeto de tipo "`Doc"', propio de la librería, que contendrá la frase con una serie de anotaciones sobre cada una de las palabras que la forman (lema, etiqueta, dependencias sintácticas, forma...).


Una vez que hemos filtrado la frase para quedarnos con las palabras que nos interesan para el anális emocional tenemos que obtener el lema de cada una de ellas. Para ello utilizaremos la librería de Pyhton \textbf{PyStemmer} que consiste en una adaptación de Snowball para Python. Snowball es un pequeño lenguaje de procesamiento que permite crear algoritmos de lematización. PyStemmer soporta el español y nos ofrece mejores resultados que Spacy a la hora de obtener los lemas de las palabras, por eso vamos a combinar ambas herramientas para procesar las palabras.


% Variable local para emacs, para  que encuentre el fichero maestro de
% compilación y funcionen mejor algunas teclas rápidas de AucTeX
%%%
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Tesis.tex"
%%% End:
